---
title: Day 23 - CHAT-AI Functionality and Model Configuration
description: Implementing the CHAT-AI functionality with different model selections and chat history management.
author: [Rajan Kumar Yadav]
date: 2024-07-03
categories: [Python, Programming]
tags: [Python, Streamlit, CHAT-AI]
pin: true
math: true
mermaid: true
---

CHAT-AI Functionality:
- Implemented the CHAT-AI section with different model selections (Groq, NVIDIA, Hugging Face).
- Configured chat history management and user prompt input handling.

Model Configuration:
- Added sidebar options for selecting models and parameters for each platform (Groq, NVIDIA, Hugging Face).
- Set up initialization for chat history and message handling.

Code Examples
```python
if selected == "CHAT-AI":
    groq_client = Groq(api_key=groq_api_key)

    # Set up NVIDIA client
    nvidia_client = OpenAI(
        base_url="https://integrate.api.nvidia.com/v1",
        api_key=nvidia_api_key
    )

    # List of available NVIDIA models
    nvidia_repo_ids = [
        "mistralai/mixtral-8x7b-instruct-v0.1",
        "meta/llama3-70b-instruct",
        "microsoft/phi-3-mini-4k-instruct"
    ]

    # List of available Hugging Face models
    hf_repo_ids = [
        "meta-llama/Meta-Llama-3-8B-Instruct",
        "mistralai/Mistral-7B-Instruct-v0.3",
        "facebook/mbart-large-50-many-to-one-mmt"
    ]

    groq_repo_ids = [
        "llama3-8b-8192",
        "mixtral-8x7b-32768",
        "whisper-large-v3",
        "gemma-7b-it"
    ]

    # Load the .env file
    dotenv_path = '.env'
    load_dotenv(dotenv_path)

    # Get the Hugging Face API token from the environment
    hf_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')

    # Set up the main Streamlit app
    st.title("CHAT-AI")

    # Sidebar configuration
    with st.sidebar:
        st.title('Platform Selection')
        model_selection = st.radio("Choose a Platform", ("Groq", "NVIDIA", "Hugging Face"))

        if model_selection == "Groq":
            st.subheader('Models and parameters')
            repo_id = st.selectbox("Select a Generative Model", groq_repo_ids)
            temperature = st.slider('Temperature', min_value=0.01, max_value=2.0, value=0.1, step=0.01)
            max_length = st.slider('Max Length', min_value=64, max_value=4096, value=512, step=8)

        if model_selection == "NVIDIA":
            st.subheader('Models and parameters')
            repo_id = st.selectbox("Select a Generative Model", nvidia_repo_ids)
            temperature = st.slider('Temperature', min_value=0.01, max_value=2.0, value=0.1, step=0.01)
            top_p = st.slider('Top P', min_value=0.01, max_value=1.0, value=0.9, step=0.01)
            max_length = st.slider('Max Length', min_value=64, max_value=4096, value=512, step=8)

        elif model_selection == "Hugging Face":
            st.subheader('Models and parameters')
            repo_id = st.selectbox("Select a Hugging Face Model", hf_repo_ids)
            temperature = st.slider('Temperature', min_value=0.01, max_value=2.0, value=0.7, step=0.01)
            max_length = st.slider('Max Length', min_value=64, max_value=4096, value=512, step=8)

            if hf_api_token is None:
                st.error("API token not found in environment. Please set HUGGINGFACEHUB_API_TOKEN in your .env file.")
                st.stop()

    # Initialize chat history
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = {"Groq": [], "NVIDIA": [], "Hugging Face": []}

    # Display or clear chat messages
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    def clear_chat_history():
        st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
        st.session_state.chat_history = {"Groq": [], "NVIDIA": [], "Hugging Face": []}
    st.sidebar.button('Clear Chat History', on_click=clear_chat_history, key="chat_clear")

    # User-provided prompt
    if prompt := st.chat_input("Enter your question here..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        if model_selection == "Groq":
            # Generate response using Groq model
            chat_completion = groq_client.chat.completions.create(
                messages=st.session_state.chat_history["Groq"] + [{"role": "user", "content": prompt}],
                model=repo_id,
                max_tokens=max_length,
                temperature=temperature
            )
            response = chat_completion.choices[0].message.content
            st.session_state.chat_history["Groq"].append({"role": "user", "content": prompt})
            st.session_state.chat_history["Groq"].append({"role": "assistant", "content": response})

        elif model_selection == "NVIDIA":
            # Directly query the NVIDIA model
            completion = nvidia_client.chat.completions.create(
                model=repo_id,
                messages=st.session_state.chat_history["NVIDIA"] + [{"role": "user", "content": prompt}],
                max_tokens=max_length,
                temperature=temperature,
                top_p=top_p,
                frequency_penalty=0,
                presence_penalty=0
            )
            response = completion.choices[0].message.content
            st.session_state.chat_history["NVIDIA"].append({"role": "user", "content": prompt})
            st.session_state.chat_history["NVIDIA"].append({"role": "assistant", "content": response})

        elif model_selection == "Hugging Face":
            # Query Hugging Face model using API endpoint
            hf_client = HuggingFaceEndpoint(repo_id=repo_id, task="chat", api_key=hf_api_token)
            response = hf_client(prompt, max_length=max_length, temperature=temperature)
            st.session_state.chat_history["Hugging Face"].append({"role": "user", "content": prompt})
            st.session_state.chat_history["Hugging Face"].append({"role": "assistant", "content": response})

        # Add response to chat messages
        st.session_state.messages.append({"role": "assistant", "content": response})
        with st.chat_message("assistant"):
            st.write(response)

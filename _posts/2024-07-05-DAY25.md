---
title: Day 25 - AI-MCQ and IMG-GEN Functionalities
description: Implementing AI-MCQ functionality for multiple-choice questions generation and IMG-GEN for image generation.
author: [Rajan Kumar Yadav]
date: 2024-07-05
categories: [Python, Programming]
tags: [Python, Streamlit, AI-MCQ, IMG-GEN]
pin: true
math: true
mermaid: true
---

AI-MCQ Functionality:
- Implemented the AI-MCQ section for generating multiple-choice questions.
- Integrated different models for generating MCQs based on user input.

IMG-GEN Functionality:
- Implemented the IMG-GEN section for generating images using NVIDIA and Hugging Face models.
- Set up image generation parameters and display of generated images.

Code Examples
```python
elif selected == "AI-MCQ":
  import os
  import sys
  import json
  import traceback
  import pandas as pd
  import PyPDF2
  import streamlit as st
  
  sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
  from utils import read_file, get_table_data
  
  from langchain_groq import ChatGroq as Groq
  from langchain.prompts import PromptTemplate
  from langchain.chains import LLMChain
  
  
  # Initialize session state if not already done
  if "quiz" not in st.session_state:
      st.session_state.quiz = None
  if "selected_answers" not in st.session_state:
      st.session_state.selected_answers = {}
  if "score" not in st.session_state:
      st.session_state.score = 0
  
  client = Groq(api_key="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")
  
  quiz_generation_template = """
  Text: {text}
  You are an expert MCQ maker. Given the above text, it is your job to create a quiz of {number} multiple choice questions for {subject} students in {tone} tone.
  Make sure the questions are not repeated and check all the questions to be conforming the text as well.
  Make sure to format your response like RESPONSE_JSON below and use it as a guide. Ensure to make {number} MCQs.
  ### RESPONSE_JSON
  {response_json}
  """
  
  quiz_generation_prompt = PromptTemplate(
      input_variables=["text", "number", "subject", "tone", "response_json"],
      template=quiz_generation_template
  )
  
  quiz_chain = LLMChain(
      llm=client,
      prompt=quiz_generation_prompt,
      output_key="quiz",
      verbose=True
  )
  
  def extract_text_from_pdf(file):
      text = ""
      try:
          reader = PyPDF2.PdfReader(file)
          for page_num in range(len(reader.pages)):
              page = reader.pages[page_num]
              text += page.extract_text()
      except Exception as e:
          st.error(f"Error reading PDF: {e}")
      return text
  
  def load_response_json(file_path):
      try:
          with open(file_path, 'r') as f:
              response_json = json.load(f)
          return json.dumps(response_json)
      except Exception as e:
          st.error(f"Error loading response.json: {e}")
          return None
  
  def main():
      st.title("MCQ Generator")
  
      with st.sidebar:
          st.header("Input Details")
          
          pdf_file = st.file_uploader("Upload PDF", type=["pdf"])
          if pdf_file is not None:
              text = extract_text_from_pdf(pdf_file)
              st.text_area("Extracted text:", text, height=300)
  
          number = st.number_input("Number of MCQs to generate:", min_value=1, max_value=20, value=3)
          subject = st.text_input("Subject of the quiz:")
          tone = st.selectbox("Tone of the quiz:", ["formal", "informal"])
          
          if st.button("Generate Quiz"):
              response_json = load_response_json("response.json")
              
              if response_json:
                  try:
                      result = quiz_chain.invoke(
                          {"text": text, "number": number, "subject": subject, "tone": tone, "response_json": response_json}
                      )
  
                      try:
                          quiz = json.loads(result["quiz"])
                          st.session_state.quiz = quiz
                      except json.JSONDecodeError as e:
                          st.error(f"Error decoding quiz JSON: {e}")
                          st.session_state.quiz = None
                      
                      st.session_state.selected_answers = {}
                      st.session_state.score = 0
  
                  except Exception as e:
                      st.error(f"An error occurred: {traceback.format_exc()}")
  
      if st.session_state.quiz:
          st.subheader("Generated Quiz")
          for q in st.session_state.quiz.values():
              st.write(f"Q{q['no']}: {q['mcq']}")
              for opt, choice in q['options'].items():
                  st.write(f"  {opt}. {choice}")
              
              selected_answer = st.session_state.selected_answers.get(q['no'], None)
              user_answer = st.radio(f"Your answer for Q{q['no']}:", list(q['options'].keys()), key=q['no'], index=list(q['options'].keys()).index(selected_answer) if selected_answer else None)
              st.session_state.selected_answers[q['no']] = user_answer
  
          if st.button("Check Answers"):
              st.session_state.score = 0
              for q in st.session_state.quiz.values():
                  user_answer = st.session_state.selected_answers.get(q['no'])
                  if user_answer == q['correct']:
                      st.write(f"Q{q['no']}: Correct!")
                      st.session_state.score += 1
                  else:
                      st.write(f"Q{q['no']}: Incorrect! The correct answer is {q['correct']}.")
  
          st.title("Quiz Score")
          st.write(f"Your score: {st.session_state.score} / {len(st.session_state.quiz)}")
  
  if __name__ == "__main__":
      main()

elif selected == "IMG-GEN":
    st.title("IMG-GEN")

    # User-provided input for image generation
    img_prompt = st.text_input("Enter a description for image generation:")
    if st.button("Generate Image"):
        if model_selection == "NVIDIA":
            # Generate image using NVIDIA model
            nvidia_client = OpenAI(api_key=nvidia_api_key)
            img_completion = nvidia_client.images.generate(
                prompt=img_prompt,
                model=repo_id,
                max_tokens=max_length,
                temperature=temperature,
                top_p=top_p
            )
            image_data = base64.b64decode(img_completion.data[0])
            st.image(image_data, caption="Generated Image")

        elif model_selection == "Hugging Face":
            # Generate image using Hugging Face model
            hf_client = HuggingFaceEndpoint(repo_id=repo_id, task="image-generation", api_key=hf_api_token)
            response = hf_client(img_prompt, max_length=max_length, temperature=temperature)
            image_data = base64.b64decode(response)
            st.image(image_data, caption="Generated Image")
